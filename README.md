# Scrago çˆ¬è™«æ¡†æ¶
# æœ¬æ–‡æ¡£ç”±aiç”Ÿæˆï¼Œå¦‚æœ‰é”™è¯¯ï¼Œè¯·åŠæ—¶æŒ‡æ­£ï¼Œç°é˜¶æ®µä¸ä¿è¯ç¨³å®šè¿è¡Œï¼Œç›®å‰ä¸ºå¼€å‘é˜¶æ®µ

<div align="center">

```
 _______ _______ _______ _______ _______ _______ 
(  ____ (  ____ (  ____ (  ___  (  ____ (  ___  )
| (    \| (    \| (    )| (   ) | (    \| (   ) |
| (_____| |     | (____)| (___) | |     | |   | |
(_____  | |     |     __|  ___  | | ____| |   | |
      ) | |     | (\ (  | (   ) | | \_  | |   | |
/\____) | (____/| ) \ \_| )   ( | (___) | (___) |
\_______(_______|/   \__|/     \(_______(_______)
                                                 
                  
```

**ä¸€ä¸ªåŸºäº Go è¯­è¨€å¼€å‘çš„é«˜æ€§èƒ½çˆ¬è™«æ¡†æ¶**

å‚è€ƒ Scrapy çš„æ¶æ„è®¾è®¡ï¼Œæ”¯æŒå¹¶å‘ã€å¼‚æ­¥ã€åˆ†å¸ƒå¼ã€åŠ¨æ€åŠ è½½æ’ä»¶ç­‰åŠŸèƒ½

[![Go Version](https://img.shields.io/badge/Go-1.19+-blue.svg)](https://golang.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Build Status](https://img.shields.io/badge/Build-Passing-brightgreen.svg)](https://github.com/bgspider/scrago)

</div>

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸš€ é«˜æ€§èƒ½æ¶æ„
- **å¹¶å‘å¤„ç†**: åŸºäº Go åç¨‹çš„é«˜å¹¶å‘çˆ¬å–
- **å¼‚æ­¥ I/O**: éé˜»å¡ç½‘ç»œè¯·æ±‚å¤„ç†
- **å†…å­˜ä¼˜åŒ–**: æ™ºèƒ½å†…å­˜ç®¡ç†å’Œåƒåœ¾å›æ”¶
- **è´Ÿè½½å‡è¡¡**: è‡ªåŠ¨è¯·æ±‚åˆ†å‘å’Œèµ„æºè°ƒåº¦

### ğŸ”§ çµæ´»æ‰©å±•
- **æ’ä»¶åŒ–è®¾è®¡**: æ”¯æŒè‡ªå®šä¹‰ä¸­é—´ä»¶å’Œç®¡é“
- **æ¨¡å—åŒ–æ¶æ„**: æ¾è€¦åˆç»„ä»¶è®¾è®¡
- **çƒ­æ’æ‹”**: è¿è¡Œæ—¶åŠ¨æ€åŠ è½½æ’ä»¶
- **API å‹å¥½**: æ¸…æ™°çš„æ‰©å±•æ¥å£

### ğŸ¯ å¼ºå¤§çš„æ•°æ®æå–
- **CSS é€‰æ‹©å™¨**: ç±»ä¼¼ jQuery çš„å…ƒç´ é€‰æ‹©
- **XPath è¡¨è¾¾å¼**: å¼ºå¤§çš„ XML è·¯å¾„è¯­è¨€
- **æ­£åˆ™è¡¨è¾¾å¼**: çµæ´»çš„æ–‡æœ¬æ¨¡å¼åŒ¹é…
- **JSON è§£æ**: åŸç”Ÿ JSON æ•°æ®å¤„ç†

### ğŸ“Š å¤šæ ·åŒ–è¾“å‡º
- **æ ¼å¼æ”¯æŒ**: JSONã€CSVã€XMLã€YAML
- **å­˜å‚¨åç«¯**: æœ¬åœ°æ–‡ä»¶ã€FTPã€S3ã€æ•°æ®åº“
- **å®æ—¶æµ**: æ”¯æŒæ•°æ®æµå¼å¤„ç†
- **æ‰¹é‡å¤„ç†**: é«˜æ•ˆçš„æ‰¹é‡æ•°æ®æ“ä½œ

### ğŸ›¡ï¸ åçˆ¬è™«å¯¹æŠ—
- **æ™ºèƒ½å»¶è¿Ÿ**: éšæœºåŒ–è¯·æ±‚é—´éš”
- **ä»£ç†è½®æ¢**: è‡ªåŠ¨ä»£ç†æ± ç®¡ç†
- **User-Agent**: æµè§ˆå™¨æ ‡è¯†è½®æ¢
- **ä¼šè¯ç®¡ç†**: Cookie å’Œä¼šè¯ä¿æŒ

## ğŸ“Š æ•°æ®ç®¡é“ç³»ç»Ÿ

### å†…ç½®ç®¡é“

| ç®¡é“ç±»å‹ | åŠŸèƒ½æè¿° | é…ç½®ç¤ºä¾‹ |
|---------|----------|----------|
| **JSONPipeline** | JSON æ ¼å¼è¾“å‡º | `OUTPUT_FORMAT: "json", OUTPUT_FILE: "data.json"` |
| **CSVPipeline** | CSV æ ¼å¼è¾“å‡º | `OUTPUT_FORMAT: "csv", CSV_DELIMITER: ","` |
| **XMLPipeline** | XML æ ¼å¼è¾“å‡º | `OUTPUT_FORMAT: "xml", XML_ROOT: "items"` |
| **DatabasePipeline** | æ•°æ®åº“å­˜å‚¨ | `DB_URL: "mysql://user:pass@host/db"` |
| **FilePipeline** | æ–‡ä»¶ä¸‹è½½ç®¡é“ | `FILES_STORE: "./downloads"` |
| **ImagePipeline** | å›¾ç‰‡ä¸‹è½½ç®¡é“ | `IMAGES_STORE: "./images", IMAGES_THUMBS: true` |
| **DuplicatesPipeline** | å»é‡å¤„ç† | `DUPEFILTER_CLASS: "RFPDupeFilter"` |
| **ValidationPipeline** | æ•°æ®éªŒè¯ | `VALIDATION_RULES: "rules.json"` |

### è‡ªå®šä¹‰ç®¡é“

```go
// æ•°æ®æ¸…æ´—ç®¡é“
type DataCleaningPipeline struct {
    stats *stats.Stats
}

func (p *DataCleaningPipeline) ProcessItem(item interface{}, spider Spider) (interface{}, error) {
    // ç±»å‹æ–­è¨€
    data, ok := item.(map[string]interface{})
    if !ok {
        return nil, fmt.Errorf("invalid item type")
    }
    
    // æ•°æ®æ¸…æ´—
    if title, exists := data["title"]; exists {
        // æ¸…ç†æ ‡é¢˜
        data["title"] = strings.TrimSpace(title.(string))
        data["title"] = regexp.MustCompile(`\s+`).ReplaceAllString(data["title"].(string), " ")
    }
    
    // æ•°æ®éªŒè¯
    if err := p.validateItem(data); err != nil {
        p.stats.IncCounter("pipeline/validation_failed")
        return nil, err
    }
    
    // æ•°æ®è½¬æ¢
    data["processed_at"] = time.Now().Unix()
    data["spider_name"] = spider.Name()
    
    p.stats.IncCounter("pipeline/items_processed")
    return data, nil
}

func (p *DataCleaningPipeline) Open(spider Spider) error {
    p.stats = stats.GetStats()
    log.Printf("DataCleaningPipeline opened for spider: %s", spider.Name())
    return nil
}

func (p *DataCleaningPipeline) Close(spider Spider) error {
    processed := p.stats.GetCounter("pipeline/items_processed")
    failed := p.stats.GetCounter("pipeline/validation_failed")
    log.Printf("DataCleaningPipeline closed. Processed: %d, Failed: %d", processed, failed)
    return nil
}

// æ•°æ®åº“å­˜å‚¨ç®¡é“
type DatabasePipeline struct {
    db     *sql.DB
    stmt   *sql.Stmt
    config *DatabaseConfig
}

func (p *DatabasePipeline) ProcessItem(item interface{}, spider Spider) (interface{}, error) {
    data := item.(map[string]interface{})
    
    // æ‰§è¡Œæ’å…¥æ“ä½œ
    _, err := p.stmt.Exec(
        data["title"],
        data["content"], 
        data["url"],
        data["created_at"],
    )
    
    if err != nil {
        return nil, fmt.Errorf("database insert failed: %v", err)
    }
    
    return item, nil
}

func (p *DatabasePipeline) Open(spider Spider) error {
    // è¿æ¥æ•°æ®åº“
    db, err := sql.Open(p.config.Driver, p.config.DSN)
    if err != nil {
        return err
    }
    
    p.db = db
    
    // å‡†å¤‡è¯­å¥
    p.stmt, err = db.Prepare(`
        INSERT INTO items (title, content, url, created_at) 
        VALUES (?, ?, ?, ?)
    `)
    
    return err
}

func (p *DatabasePipeline) Close(spider Spider) error {
    if p.stmt != nil {
        p.stmt.Close()
    }
    if p.db != nil {
        p.db.Close()
    }
    return nil
}
```

### ç®¡é“é…ç½®

```go
// ç®¡é“æ³¨å†Œå’Œé…ç½®
settings := &settings.Settings{
    Pipelines: []string{
        "validation",     // æ•°æ®éªŒè¯
        "cleaning",       // æ•°æ®æ¸…æ´—
        "duplicates",     // å»é‡å¤„ç†
        "database",       // æ•°æ®åº“å­˜å‚¨
        "json",          // JSON è¾“å‡º
    },
    
    // ç®¡é“ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
    PipelinePriority: map[string]int{
        "validation": 100,
        "cleaning":   200,
        "duplicates": 300,
        "database":   400,
        "json":       500,
    },
}
```

## ğŸ”Œ æ‰©å±•ç³»ç»Ÿ

### å†…ç½®æ‰©å±•

| æ‰©å±•åç§° | åŠŸèƒ½æè¿° | é…ç½®é€‰é¡¹ |
|---------|----------|----------|
| **StatsExtension** | ç»Ÿè®¡ä¿¡æ¯æ”¶é›† | `STATS_CLASS: "MemoryStats"` |
| **LogStatsExtension** | æ—¥å¿—ç»Ÿè®¡è¾“å‡º | `LOGSTATS_INTERVAL: 60` |
| **MemoryUsageExtension** | å†…å­˜ä½¿ç”¨ç›‘æ§ | `MEMUSAGE_LIMIT_MB: 1024` |
| **SpiderStateExtension** | çˆ¬è™«çŠ¶æ€ç®¡ç† | `SPIDER_STATE_ENABLED: true` |
| **AutoThrottleExtension** | è‡ªåŠ¨é™é€Ÿ | `AUTOTHROTTLE_ENABLED: true` |
| **CloseDomainExtension** | åŸŸåå…³é—­æ£€æµ‹ | `CLOSEDOMAIN_ENABLED: true` |

### è‡ªå®šä¹‰æ‰©å±•

```go
// æ€§èƒ½ç›‘æ§æ‰©å±•
type PerformanceExtension struct {
    startTime    time.Time
    requestCount int64
    errorCount   int64
    stats        *stats.Stats
}

func (e *PerformanceExtension) SpiderOpened(spider Spider) {
    e.startTime = time.Now()
    e.stats = stats.GetStats()
    log.Printf("Performance monitoring started for spider: %s", spider.Name())
}

func (e *PerformanceExtension) SpiderClosed(spider Spider, reason string) {
    duration := time.Since(e.startTime)
    requests := e.stats.GetCounter("downloader/request_count")
    errors := e.stats.GetCounter("downloader/exception_count")
    
    // è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    rps := float64(requests) / duration.Seconds()
    errorRate := float64(errors) / float64(requests) * 100
    
    log.Printf("Spider %s finished. Duration: %v, RPS: %.2f, Error Rate: %.2f%%", 
        spider.Name(), duration, rps, errorRate)
    
    // å‘é€æ€§èƒ½æŠ¥å‘Š
    e.sendPerformanceReport(spider.Name(), duration, rps, errorRate)
}

func (e *PerformanceExtension) RequestScheduled(request *http.Request, spider Spider) {
    atomic.AddInt64(&e.requestCount, 1)
}

func (e *PerformanceExtension) RequestDropped(request *http.Request, spider Spider, reason string) {
    atomic.AddInt64(&e.errorCount, 1)
}

// é‚®ä»¶é€šçŸ¥æ‰©å±•
type EmailNotificationExtension struct {
    config *EmailConfig
}

func (e *EmailNotificationExtension) SpiderOpened(spider Spider) {
    e.sendEmail(
        "Spider Started", 
        fmt.Sprintf("Spider %s has started crawling", spider.Name()),
    )
}

func (e *EmailNotificationExtension) SpiderClosed(spider Spider, reason string) {
    stats := stats.GetStats()
    itemCount := stats.GetCounter("item_scraped_count")
    
    message := fmt.Sprintf(
        "Spider %s finished with reason: %s\nItems scraped: %d",
        spider.Name(), reason, itemCount,
    )
    
    e.sendEmail("Spider Finished", message)
}

// æ‰©å±•æ³¨å†Œ
func init() {
    extensions.Register("performance", &PerformanceExtension{})
    extensions.Register("email_notification", &EmailNotificationExtension{})
}
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ“¦ å®‰è£…

#### æ–¹å¼ä¸€ï¼šä½¿ç”¨ Go Modulesï¼ˆæ¨èï¼‰

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/bgspider/scrago.git
cd scrago

# å®‰è£…ä¾èµ–
go mod tidy

# æ„å»ºé¡¹ç›®
go build -o scrago ./cmd/scrago
```

#### æ–¹å¼äºŒï¼šç›´æ¥ä¸‹è½½(è‡ªå·±ç¼–è¯‘ï¼Œä½œè€…æ²¡æœ‰ç¼–è¯‘å…¶ä»–ç‰ˆæœ¬)

```bash
# ä¸‹è½½é¢„ç¼–è¯‘äºŒè¿›åˆ¶æ–‡ä»¶
wget https://github.com/bgspider/scrago/releases/latest/download/scrago-linux-amd64
chmod +x scrago-linux-amd64
mv scrago-linux-amd64 /usr/local/bin/scrago
```

### ğŸ¯ å‘½ä»¤è¡Œå·¥å…·

Scrago æä¾›äº†å¼ºå¤§çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œè®©æ‚¨å¿«é€Ÿä¸Šæ‰‹ï¼š

```bash
# æŸ¥çœ‹å¸®åŠ©
scrago --help

# æŸ¥çœ‹å¯ç”¨çˆ¬è™«
scrago list

# è¿è¡Œçˆ¬è™«
scrago crawl douban_movie

# åˆ›å»ºæ–°é¡¹ç›®
scrago startproject myproject

# ç”Ÿæˆæ–°çˆ¬è™«
scrago genspider quotes quotes.toscrape.com
```

### ğŸ’» åŸºç¡€ä½¿ç”¨

#### ç®€å•ç¤ºä¾‹

```go
package main

import (
    "fmt"
    "scrago/engine"
    "scrago/spider"
    "scrago/pipeline"
    "scrago/middleware"
)

func main() {
    // åˆ›å»ºçˆ¬è™«å¼•æ“
    e := engine.NewEngine()
    
    // æ·»åŠ ä¸­é—´ä»¶
    e.AddMiddleware(middleware.NewUserAgentMiddleware())
    e.AddMiddleware(middleware.NewDelayMiddleware())
    
    // æ·»åŠ æ•°æ®ç®¡é“
    e.AddPipeline(pipeline.NewConsolePipeline())
    e.AddPipeline(pipeline.NewJSONPipeline("output.json"))
    
    // è®¾ç½®å¹¶å‘æ•°
    e.SetConcurrency(16)
    
    // åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
    s := spider.NewExampleSpider()
    
    fmt.Println("ğŸ•·ï¸ å¼€å§‹çˆ¬å–...")
    e.Run(s)
    fmt.Println("âœ… çˆ¬å–å®Œæˆï¼")
}
```

#### è‡ªå®šä¹‰çˆ¬è™«

åˆ›å»ºä¸€ä¸ªæ–°é—»çˆ¬è™«ç¤ºä¾‹ï¼š

```go
package main

import (
    "fmt"
    "strings"
    "scrago/spider"
    "scrago/response"
    "scrago/request"
)

// NewsItem æ–°é—»æ•°æ®ç»“æ„
type NewsItem struct {
    Title       string `json:"title"`
    Content     string `json:"content"`
    Author      string `json:"author"`
    PublishTime string `json:"publish_time"`
    URL         string `json:"url"`
    Tags        []string `json:"tags"`
}

// NewsSpider æ–°é—»çˆ¬è™«
type NewsSpider struct {
    *spider.BaseSpider
}

// NewNewsSpider åˆ›å»ºæ–°é—»çˆ¬è™«
func NewNewsSpider() *NewsSpider {
    base := spider.NewBaseSpider("news", []string{
        "https://news.example.com",
        "https://tech.example.com",
    })
    return &NewsSpider{BaseSpider: base}
}

// Parse è§£æé¦–é¡µï¼Œæå–æ–‡ç« é“¾æ¥
func (s *NewsSpider) Parse(resp *response.Response) []interface{} {
    results := make([]interface{}, 0)
    
    // æå–æ–‡ç« é“¾æ¥
    articleLinks := resp.CSS("article a.title").Attrs("href")
    
    for _, link := range articleLinks {
        // åˆ›å»ºæ–°è¯·æ±‚ï¼ŒæŒ‡å®šå›è°ƒå‡½æ•°
        req := resp.Follow(link)
        req.Callback = s.ParseArticle
        results = append(results, req)
    }
    
    // å¤„ç†åˆ†é¡µ
    nextPage := resp.CSS("a.next-page").Attr("href")
    if nextPage != "" {
        nextReq := resp.Follow(nextPage)
        nextReq.Callback = s.Parse
        results = append(results, nextReq)
    }
    
    return results
}

// ParseArticle è§£ææ–‡ç« è¯¦æƒ…é¡µ
func (s *NewsSpider) ParseArticle(resp *response.Response) []interface{} {
    results := make([]interface{}, 0)
    
    // æå–æ–‡ç« æ•°æ®
    item := &NewsItem{
        Title:       resp.CSS("h1.article-title").Text(),
        Content:     resp.CSS("div.article-content").Text(),
        Author:      resp.CSS("span.author").Text(),
        PublishTime: resp.CSS("time.publish-date").Attr("datetime"),
        URL:         resp.URL,
    }
    
    // æå–æ ‡ç­¾
    tagElements := resp.CSS("div.tags a")
    for _, tag := range tagElements.Texts() {
        item.Tags = append(item.Tags, strings.TrimSpace(tag))
    }
    
    // æ•°æ®éªŒè¯
    if item.Title != "" && item.Content != "" {
        results = append(results, item)
        fmt.Printf("ğŸ“° æå–æ–‡ç« : %s\n", item.Title)
    }
    
    return results
}

// ä½¿ç”¨ç¤ºä¾‹
func main() {
    e := engine.NewEngine()
    
    // é…ç½®å¼•æ“
    e.SetConcurrency(8)
    e.SetDownloadDelay(time.Second * 2)
    
    // æ·»åŠ ç®¡é“
    e.AddPipeline(pipeline.NewJSONPipeline("news.json"))
    
    // è¿è¡Œçˆ¬è™«
    spider := NewNewsSpider()
    e.Run(spider)
}
```

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„

### ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Scrago æ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CLI Tool  â”‚  Web UI  â”‚  API Server  â”‚  Monitoring        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      Engine (å¼•æ“)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Scheduler â”‚ Downloader â”‚ Middleware â”‚ Pipeline â”‚ Spider    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Request  â”‚  Response  â”‚  Selector  â”‚ Settings â”‚ Stats     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. ğŸš€ çˆ¬è™«å¼•æ“ (Engine)

å¼•æ“æ˜¯æ•´ä¸ªæ¡†æ¶çš„å¤§è„‘ï¼Œåè°ƒæ‰€æœ‰ç»„ä»¶çš„å·¥ä½œï¼š

```go
type Engine struct {
    scheduler   Scheduler     // è¯·æ±‚è°ƒåº¦å™¨
    downloader  Downloader    // ä¸‹è½½å™¨
    middlewares []Middleware  // ä¸­é—´ä»¶é“¾
    pipelines   []Pipeline    // æ•°æ®ç®¡é“
    settings    *Settings     // é…ç½®ç®¡ç†
    stats       *Stats        // ç»Ÿè®¡ä¿¡æ¯
}

// æ ¸å¿ƒåŠŸèƒ½
func (e *Engine) Run(spider Spider) error
func (e *Engine) SetConcurrency(n int)
func (e *Engine) AddMiddleware(m Middleware)
func (e *Engine) AddPipeline(p Pipeline)
```

**ä¸»è¦èŒè´£ï¼š**
- ğŸ”„ åè°ƒå„ç»„ä»¶å·¥ä½œæµç¨‹
- ğŸ“Š ç®¡ç†å¹¶å‘å’Œèµ„æºåˆ†é…
- ğŸ“ˆ æ”¶é›†è¿è¡Œç»Ÿè®¡ä¿¡æ¯
- âš™ï¸ å¤„ç†é…ç½®å’Œè®¾ç½®
- ğŸ›¡ï¸ å¼‚å¸¸å¤„ç†å’Œæ¢å¤

### 2. ğŸ•·ï¸ çˆ¬è™« (Spider)

å®šä¹‰å…·ä½“çš„çˆ¬å–é€»è¾‘å’Œæ•°æ®æå–è§„åˆ™ï¼š

```go
type Spider interface {
    Name() string                                    // çˆ¬è™«åç§°
    StartRequests() []*request.Request              // åˆå§‹è¯·æ±‚
    Parse(resp *response.Response) []interface{}    // è§£æå“åº”
}

// é«˜çº§çˆ¬è™«æ¥å£
type AdvancedSpider interface {
    Spider
    AllowedDomains() []string                       // å…è®¸çš„åŸŸå
    CustomSettings() map[string]interface{}         // è‡ªå®šä¹‰è®¾ç½®
    HandleError(req *request.Request, err error)    // é”™è¯¯å¤„ç†
}
```

### 3. ğŸ“‹ è°ƒåº¦å™¨ (Scheduler)

æ™ºèƒ½çš„è¯·æ±‚è°ƒåº¦å’Œé˜Ÿåˆ—ç®¡ç†ï¼š

```go
type Scheduler interface {
    Push(req *request.Request) error    // æ·»åŠ è¯·æ±‚
    Pop() (*request.Request, error)     // è·å–è¯·æ±‚
    Size() int                          // é˜Ÿåˆ—å¤§å°
    Close() error                       // å…³é—­è°ƒåº¦å™¨
}

// æ”¯æŒçš„è°ƒåº¦ç­–ç•¥
- FIFO: å…ˆè¿›å…ˆå‡º (é»˜è®¤)
- LIFO: åè¿›å…ˆå‡º
- Priority: ä¼˜å…ˆçº§é˜Ÿåˆ—
- Random: éšæœºè°ƒåº¦
- Custom: è‡ªå®šä¹‰ç­–ç•¥
```

### 4. ğŸŒ ä¸‹è½½å™¨ (Downloader)

é«˜æ€§èƒ½çš„ HTTP å®¢æˆ·ç«¯ï¼š

```go
type Downloader interface {
    Download(req *request.Request) (*response.Response, error)
    SetProxy(proxy string) error
    SetTimeout(timeout time.Duration)
    SetRetryTimes(times int)
}

// ç‰¹æ€§æ”¯æŒ
- HTTP/HTTPS åè®®
- ä»£ç†æœåŠ¡å™¨æ”¯æŒ
- è‡ªåŠ¨é‡è¯•æœºåˆ¶
- Cookie å’Œä¼šè¯ç®¡ç†
- è‡ªå®šä¹‰è¯·æ±‚å¤´
- æ–‡ä»¶ä¸Šä¼ ä¸‹è½½
- å‹ç¼©ä¼ è¾“ (gzip, deflate)
```

### 5. ğŸ¯ é€‰æ‹©å™¨ (Selector)

å¼ºå¤§çš„æ•°æ®æå–å·¥å…·é›†ï¼š

```go
// CSS é€‰æ‹©å™¨ - ç±»ä¼¼ jQuery
titles := resp.CSS("h1.title").Texts()
links := resp.CSS("a[href]").Attrs("href")
first := resp.CSS("div.content").First().Text()

// XPath è¡¨è¾¾å¼ - å¼ºå¤§çš„ XML è·¯å¾„
nodes := resp.XPath("//div[@class='item']//text()").Texts()
attrs := resp.XPath("//a/@href").Strings()

// æ­£åˆ™è¡¨è¾¾å¼ - çµæ´»çš„æ¨¡å¼åŒ¹é…
emails := resp.Regex(`[\w\.-]+@[\w\.-]+\.\w+`).Strings()
phones := resp.Regex(`\d{3}-\d{3}-\d{4}`).Strings()

// JSON æ•°æ®æå–
data := resp.JSON("data.items[*].name").Strings()
```

### 6. ğŸ”Œ ä¸­é—´ä»¶ (Middleware)

å¯æ’æ‹”çš„è¯·æ±‚/å“åº”å¤„ç†ç»„ä»¶ï¼š

```go
type Middleware interface {
    ProcessRequest(req *request.Request) error
    ProcessResponse(resp *response.Response) error
}

// å†…ç½®ä¸­é—´ä»¶
- UserAgentMiddleware: æµè§ˆå™¨æ ‡è¯†è½®æ¢
- ProxyMiddleware: ä»£ç†æœåŠ¡å™¨è½®æ¢  
- DelayMiddleware: æ™ºèƒ½å»¶è¿Ÿæ§åˆ¶
- RetryMiddleware: å¤±è´¥é‡è¯•å¤„ç†
- CacheMiddleware: å“åº”ç¼“å­˜ç®¡ç†
- CookieMiddleware: Cookie è‡ªåŠ¨ç®¡ç†
- AuthMiddleware: èº«ä»½è®¤è¯å¤„ç†
- RobotsTxtMiddleware: robots.txt éµå®ˆ
```

### 7. ğŸ“Š æ•°æ®ç®¡é“ (Pipeline)

æ•°æ®å¤„ç†å’Œå­˜å‚¨çš„æµæ°´çº¿ï¼š

```go
type Pipeline interface {
    ProcessItem(item interface{}) (interface{}, error)
    Open() error
    Close() error
}

// å†…ç½®ç®¡é“
- ConsolePipeline: æ§åˆ¶å°è¾“å‡º
- JSONPipeline: JSON æ–‡ä»¶å­˜å‚¨
- CSVPipeline: CSV è¡¨æ ¼å­˜å‚¨
- XMLPipeline: XML æ–‡æ¡£å­˜å‚¨
- DatabasePipeline: æ•°æ®åº“å­˜å‚¨
- FilterPipeline: æ•°æ®è¿‡æ»¤æ¸…æ´—
- TransformPipeline: æ•°æ®æ ¼å¼è½¬æ¢
- ValidationPipeline: æ•°æ®éªŒè¯æ£€æŸ¥
```

## âš™ï¸ é…ç½®é€‰é¡¹

### åŸºç¡€é…ç½®

```go
// åˆ›å»ºé…ç½®
settings := &settings.Settings{
    // ğŸš€ æ€§èƒ½é…ç½®
    Concurrency:        16,                     // å¹¶å‘åç¨‹æ•°
    DownloadDelay:      time.Second,            // ä¸‹è½½å»¶è¿Ÿ
    RandomizeDelay:     true,                   // éšæœºåŒ–å»¶è¿Ÿ (0.5-1.5å€)
    AutoThrottle:       true,                   // è‡ªåŠ¨é™é€Ÿ
    
    // ğŸŒ ç½‘ç»œé…ç½®  
    UserAgent:          "Scrago/1.0",          // User-Agent
    Timeout:            30 * time.Second,       // è¯·æ±‚è¶…æ—¶
    KeepAlive:          true,                   // è¿æ¥ä¿æŒ
    MaxIdleConns:       100,                    // æœ€å¤§ç©ºé—²è¿æ¥
    
    // ğŸ”„ é‡è¯•é…ç½®
    RetryTimes:         3,                      // é‡è¯•æ¬¡æ•°
    RetryHTTPCodes:     []int{500, 502, 503},   // é‡è¯•çŠ¶æ€ç 
    RetryDelay:         time.Second * 2,        // é‡è¯•å»¶è¿Ÿ
    
    // ğŸ›¡ï¸ å®‰å…¨é…ç½®
    RobotsTxtObey:      true,                   // éµå®ˆ robots.txt
    AllowedDomains:     []string{},             // å…è®¸çš„åŸŸå
    DeniedDomains:      []string{},             // ç¦æ­¢çš„åŸŸå
    
    // ğŸ“Š è¾“å‡ºé…ç½®
    LogLevel:           "INFO",                 // æ—¥å¿—çº§åˆ«
    StatsEnabled:       true,                   // ç»Ÿè®¡ä¿¡æ¯
    OutputFormat:       "json",                 // è¾“å‡ºæ ¼å¼
}

// åº”ç”¨é…ç½®
engine := engine.NewEngine()
engine.ApplySettings(settings)
```

### é«˜çº§é…ç½®

```go
// ä»£ç†é…ç½®
proxyConfig := &middleware.ProxyConfig{
    Proxies: []string{
        "http://proxy1:8080",
        "http://proxy2:8080",
        "socks5://proxy3:1080",
    },
    RotateMode: "random",  // random, round_robin, failover
}

// ç¼“å­˜é…ç½®
cacheConfig := &middleware.CacheConfig{
    Enabled:    true,
    TTL:        time.Hour * 24,
    MaxSize:    1000,
    Storage:    "memory",  // memory, redis, file
}

// æ•°æ®åº“é…ç½®
dbConfig := &pipeline.DatabaseConfig{
    Driver:   "mysql",
    Host:     "localhost",
    Port:     3306,
    Database: "scrago",
    Username: "root",
    Password: "password",
}
```

## ğŸ“š ç¤ºä¾‹é¡¹ç›®

### ğŸ¯ å†…ç½®çˆ¬è™«ç¤ºä¾‹

| çˆ¬è™«åç§° | ç›®æ ‡ç½‘ç«™ | æ•°æ®ç±»å‹ | è¿è¡Œå‘½ä»¤ |
|---------|---------|---------|----------|
| **douban_movie** | è±†ç“£ç”µå½± | ç”µå½±ä¿¡æ¯ | `scrago crawl douban_movie` |
| **quotes** | Quotes to Scrape | åè¨€è­¦å¥ | `scrago crawl quotes` |
| **books** | Books to Scrape | å›¾ä¹¦ä¿¡æ¯ | `scrago crawl books` |
| **news** | æ–°é—»ç½‘ç«™ | æ–°é—»æ–‡ç«  | `scrago crawl news` |

### ğŸ› ï¸ è‡ªå®šä¹‰é¡¹ç›®ç¤ºä¾‹

```bash
# åˆ›å»ºæ–°é¡¹ç›®
scrago startproject myproject
cd myproject

# ç”Ÿæˆçˆ¬è™«
scrago genspider quotes quotes.toscrape.com
scrago genspider books books.toscrape.com

# è¿è¡Œçˆ¬è™«
scrago crawl quotes -o quotes.json
scrago crawl books -o books.csv -s CONCURRENT_REQUESTS=32
```

### ğŸ“Š å®é™…åº”ç”¨åœºæ™¯

#### ğŸ›’ ç”µå•†æ•°æ®ç›‘æ§
```bash
# å•†å“ä»·æ ¼ç›‘æ§
scrago crawl ecommerce \
    --set DOWNLOAD_DELAY=2 \
    --set CONCURRENT_REQUESTS=8 \
    --output products.json \
    --log-level INFO
```

#### ğŸ“° æ–°é—»å†…å®¹èšåˆ
```bash
# å¤šæºæ–°é—»æŠ“å–
scrago crawl news \
    --set USER_AGENT_LIST=user_agents.txt \
    --set PROXY_LIST=proxies.txt \
    --output news.csv \
    --format csv
```

#### ğŸ“ˆ ç¤¾äº¤åª’ä½“åˆ†æ
```bash
# ç¤¾äº¤åª’ä½“æ•°æ®
scrago crawl social \
    --set COOKIES_ENABLED=true \
    --set SESSION_PERSISTENCE=true \
    --output social_data.json \
    --pipeline database
```

## ğŸš€ è¿è¡Œç¤ºä¾‹

### å‘½ä»¤è¡Œå·¥å…·

```bash
# æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯
scrago --help

# æŸ¥çœ‹å¯ç”¨çˆ¬è™«
scrago list

# è¿è¡Œå†…ç½®çˆ¬è™«
scrago crawl douban_movie
scrago crawl quotes -o output.json
scrago crawl books -o books.csv -s CONCURRENT_REQUESTS=16

# åˆ›å»ºæ–°é¡¹ç›®
scrago startproject myproject

# ç”Ÿæˆçˆ¬è™«æ¨¡æ¿
scrago genspider myspider example.com

# æ£€æŸ¥çˆ¬è™«è¯­æ³•
scrago check myspider

# æŸ¥çœ‹çˆ¬è™«ç»Ÿè®¡
scrago stats douban_movie
```

### ç¨‹åºåŒ–è¿è¡Œ

```bash
# ç›´æ¥è¿è¡Œ Go ç¨‹åº
go run cmd/scrago/main.go crawl douban_movie
go run cmd/scrago/main.go list

# ç¼–è¯‘åè¿è¡Œ
go build -o scrago cmd/scrago/main.go
./scrago crawl douban_movie
```

## ğŸ”§ ä¸­é—´ä»¶ç³»ç»Ÿ

### å†…ç½®ä¸­é—´ä»¶

| ä¸­é—´ä»¶ | åŠŸèƒ½æè¿° | é…ç½®ç¤ºä¾‹ |
|--------|----------|----------|
| **UserAgentMiddleware** | éšæœº User-Agent è½®æ¢ | `USER_AGENT_LIST: ["Chrome/91.0", "Firefox/89.0"]` |
| **ProxyMiddleware** | ä»£ç†æœåŠ¡å™¨æ”¯æŒ | `PROXY_LIST: ["http://proxy1:8080"]` |
| **RetryMiddleware** | æ™ºèƒ½é‡è¯•æœºåˆ¶ | `RETRY_TIMES: 3, RETRY_HTTP_CODES: [500, 502]` |
| **CacheMiddleware** | HTTP ç¼“å­˜æ”¯æŒ | `CACHE_ENABLED: true, CACHE_TTL: "24h"` |
| **RobotsTxtMiddleware** | robots.txt éµå®ˆ | `ROBOTSTXT_OBEY: true` |
| **CookieMiddleware** | Cookie ç®¡ç† | `COOKIES_ENABLED: true` |
| **CompressionMiddleware** | å“åº”å‹ç¼©å¤„ç† | `COMPRESSION_ENABLED: true` |
| **AuthMiddleware** | èº«ä»½éªŒè¯æ”¯æŒ | `AUTH_USERNAME: "user", AUTH_PASSWORD: "pass"` |

### è‡ªå®šä¹‰ä¸­é—´ä»¶

```go
// å®ç°ä¸­é—´ä»¶æ¥å£
type CustomMiddleware struct {
    config *MiddlewareConfig
}

// è¯·æ±‚é¢„å¤„ç†
func (m *CustomMiddleware) ProcessRequest(req *http.Request, spider Spider) error {
    // æ·»åŠ è‡ªå®šä¹‰è¯·æ±‚å¤´
    req.Header.Set("X-Custom-Header", "MyValue")
    req.Header.Set("X-Request-ID", generateRequestID())
    
    // è¯·æ±‚æ—¥å¿—è®°å½•
    log.Printf("Processing request: %s", req.URL.String())
    
    return nil
}

// å“åº”åå¤„ç†
func (m *CustomMiddleware) ProcessResponse(resp *http.Response, req *http.Request, spider Spider) (*http.Response, error) {
    // å“åº”çŠ¶æ€æ£€æŸ¥
    if resp.StatusCode >= 400 {
        return nil, fmt.Errorf("HTTP error: %d", resp.StatusCode)
    }
    
    // å“åº”æ—¶é—´è®°å½•
    duration := time.Since(req.Context().Value("start_time").(time.Time))
    log.Printf("Response received in %v", duration)
    
    return resp, nil
}

// å¼‚å¸¸å¤„ç†
func (m *CustomMiddleware) ProcessException(err error, req *http.Request, spider Spider) error {
    // é”™è¯¯æ—¥å¿—è®°å½•
    log.Printf("Request failed: %s, Error: %v", req.URL.String(), err)
    
    // å¯ä»¥é€‰æ‹©å¿½ç•¥æŸäº›é”™è¯¯
    if isIgnorableError(err) {
        return nil
    }
    
    return err
}

// æ³¨å†Œä¸­é—´ä»¶
func init() {
    middleware.Register("custom", &CustomMiddleware{})
}
```

### ä¸­é—´ä»¶é…ç½®

```go
// åœ¨è®¾ç½®ä¸­å¯ç”¨ä¸­é—´ä»¶
settings := &settings.Settings{
    Middlewares: []string{
        "user_agent",
        "proxy", 
        "retry",
        "cache",
        "custom",  // è‡ªå®šä¹‰ä¸­é—´ä»¶
    },
    
    // ä¸­é—´ä»¶ä¼˜å…ˆçº§
    MiddlewarePriority: map[string]int{
        "user_agent": 100,
        "proxy":      200,
        "retry":      300,
        "cache":      400,
        "custom":     500,
    },
}
```

## é¡¹ç›®ç»“æ„

```
scrago/
â”œâ”€â”€ engine/          # çˆ¬è™«å¼•æ“
â”œâ”€â”€ spider/          # çˆ¬è™«åŸºç±»å’Œç¤ºä¾‹
â”œâ”€â”€ request/         # è¯·æ±‚å¯¹è±¡
â”œâ”€â”€ response/        # å“åº”å¯¹è±¡
â”œâ”€â”€ selector/        # é€‰æ‹©å™¨å’Œæ•°æ®æå–
â”œâ”€â”€ downloader/      # ä¸‹è½½å™¨
â”œâ”€â”€ scheduler/       # è°ƒåº¦å™¨
â”œâ”€â”€ pipeline/        # æ•°æ®ç®¡é“
â”œâ”€â”€ middleware/      # ä¸­é—´ä»¶
â”œâ”€â”€ examples/        # ç¤ºä¾‹ä»£ç 
â”œâ”€â”€ main.go          # ä¸»å…¥å£
â”œâ”€â”€ go.mod           # ä¾èµ–ç®¡ç†
â””â”€â”€ README.md        # é¡¹ç›®æ–‡æ¡£
```

## ä¾èµ–åŒ…

- `github.com/PuerkitoBio/goquery`: HTML è§£æå’Œ CSS é€‰æ‹©å™¨
- `github.com/antchfx/htmlquery`: HTML XPath æ”¯æŒ
- `github.com/antchfx/xmlquery`: XML XPath æ”¯æŒ
- `github.com/antchfx/xpath`: XPath è¡¨è¾¾å¼å¼•æ“
- `golang.org/x/net`: ç½‘ç»œç›¸å…³å·¥å…·

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼æ— è®ºæ˜¯ bug æŠ¥å‘Šã€åŠŸèƒ½è¯·æ±‚ã€æ–‡æ¡£æ”¹è¿›è¿˜æ˜¯ä»£ç è´¡çŒ®ã€‚

### ğŸ› æŠ¥å‘Š Bug

å¦‚æœæ‚¨å‘ç°äº† bugï¼Œè¯·åˆ›å»ºä¸€ä¸ª issue å¹¶åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š

- **Bug æè¿°**: æ¸…æ™°ç®€æ´çš„æè¿°
- **å¤ç°æ­¥éª¤**: è¯¦ç»†çš„å¤ç°æ­¥éª¤
- **æœŸæœ›è¡Œä¸º**: æ‚¨æœŸæœ›å‘ç”Ÿä»€ä¹ˆ
- **å®é™…è¡Œä¸º**: å®é™…å‘ç”Ÿäº†ä»€ä¹ˆ
- **ç¯å¢ƒä¿¡æ¯**: Go ç‰ˆæœ¬ã€æ“ä½œç³»ç»Ÿç­‰
- **ç›¸å…³æ—¥å¿—**: é”™è¯¯æ—¥å¿—æˆ–å †æ ˆè·Ÿè¸ª

### ğŸ’¡ åŠŸèƒ½è¯·æ±‚

å¯¹äºæ–°åŠŸèƒ½å»ºè®®ï¼Œè¯·åˆ›å»ºä¸€ä¸ª issue å¹¶è¯´æ˜ï¼š

- **åŠŸèƒ½æè¿°**: æ‚¨å¸Œæœ›æ·»åŠ çš„åŠŸèƒ½
- **ä½¿ç”¨åœºæ™¯**: ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªåŠŸèƒ½
- **å®ç°å»ºè®®**: å¦‚æœæœ‰çš„è¯ï¼Œæä¾›å®ç°æ€è·¯

### ğŸ”§ ä»£ç è´¡çŒ®

1. **Fork é¡¹ç›®**
   ```bash
   git clone https://github.com/bgspiders/scrago.git
   cd scrago
   ```

2. **åˆ›å»ºåŠŸèƒ½åˆ†æ”¯**
   ```bash
   git checkout -b feature/amazing-feature
   ```

3. **å¼€å‘å’Œæµ‹è¯•**
   ```bash
   # å®‰è£…ä¾èµ–
   go mod tidy
   
   # è¿è¡Œæµ‹è¯•
   go test ./...
   
   # è¿è¡Œ linter
   golangci-lint run
   
   # æ ¼å¼åŒ–ä»£ç 
   go fmt ./...
   ```

4. **æäº¤æ›´æ”¹**
   ```bash
   git add .
   git commit -m "feat: add amazing feature"
   ```

5. **æ¨é€å¹¶åˆ›å»º PR**
   ```bash
   git push origin feature/amazing-feature
   ```

### ğŸ“ ä»£ç è§„èŒƒ

- éµå¾ª Go å®˜æ–¹ä»£ç é£æ ¼
- æ·»åŠ é€‚å½“çš„æ³¨é‡Šå’Œæ–‡æ¡£
- ç¼–å†™å•å…ƒæµ‹è¯•
- ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡
- ä½¿ç”¨æœ‰æ„ä¹‰çš„æäº¤ä¿¡æ¯

### ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
go test ./...

# è¿è¡Œç‰¹å®šåŒ…çš„æµ‹è¯•
go test ./pkg/spider

# è¿è¡Œæµ‹è¯•å¹¶æ˜¾ç¤ºè¦†ç›–ç‡
go test -cover ./...

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
go test -coverprofile=coverage.out ./...
go tool cover -html=coverage.out
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ **MIT è®¸å¯è¯** - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

```
MIT License

Copyright (c) 2024 Scrago Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸º Scrago é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ï¼

- ç‰¹åˆ«æ„Ÿè°¢ [Scrapy](https://scrapy.org/) é¡¹ç›®æä¾›çš„è®¾è®¡çµæ„Ÿ
- æ„Ÿè°¢ Go ç¤¾åŒºæä¾›çš„ä¼˜ç§€å·¥å…·å’Œåº“
- æ„Ÿè°¢æ‰€æœ‰æäº¤ issue å’Œ PR çš„è´¡çŒ®è€…

## ğŸ“ è”ç³»æˆ‘ä»¬

- **GitHub Issues**: [æäº¤é—®é¢˜](https://github.com/bgspiders/scrago/issues)
- **è®¨è®ºåŒº**: [GitHub Discussions](https://github.com/bgspiders/scrago/discussions)
- **é‚®ç®±**: scrago@example.com

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼â­**

[ğŸ  é¦–é¡µ](https://github.com/bgspiders/scrago) â€¢ 
[ğŸ“– æ–‡æ¡£](https://scrago.readthedocs.io) â€¢ 
[ğŸ› æŠ¥å‘Š Bug](https://github.com/bgspiders/scrago/issues) â€¢ 
[ğŸ’¡ åŠŸèƒ½è¯·æ±‚](https://github.com/bgspiders/scrago/issues)

</div>